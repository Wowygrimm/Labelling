{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables\n",
    "\n",
    "Here is the selected variables:\n",
    "\n",
    "- `TRIAL_INDEX`\n",
    "- `EYE_USED`\n",
    "- `CURRENT_FIX_X`\n",
    "- `CURRENT_FIX_Y`\n",
    "- `CURRENT_FIX_START`\n",
    "- `CURRENT_FIX_DURATION`\n",
    "- `NEXT_SAC_END_X`\n",
    "- `NEXT_SAC_END_Y`\n",
    "- `NEXT_SAC_AMPLITUDE`\n",
    "- `NEXT_SAC_DIRECTION`\n",
    "- `NEXT_SAC_DURATION`\n",
    "- `NEXT_SAC_ANGLE`\n",
    "- `NEXT_SAC_AVG_VELOCITY`\n",
    "\n",
    "Since the Timestamp of the events is not at the right time, we need to synchronize Mouse data and eyes data. To do such, we sent a MSG containing `TRIAL_START=XXX-XX-XX XX:XX:XX` at the begining of the trial and `TRIAL_END=XXX-XX-XX XX:XX:XX` at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_columns(data):\n",
    "    data[\"CURRENT_FIX_Y\"] = pd.to_numeric(data[\"CURRENT_FIX_Y\"].str.replace(',','.')).fillna(0)\n",
    "    data[\"CURRENT_FIX_X\"] = pd.to_numeric(data[\"CURRENT_FIX_X\"].str.replace(',','.')).fillna(0)\n",
    "    data[\"NEXT_SAC_AMPLITUDE\"] = pd.to_numeric(data[\"NEXT_SAC_AMPLITUDE\"].str.replace(\".\", \"\").str.replace(\",\", \".\")).fillna(0)\n",
    "    data[\"NEXT_SAC_END_X\"] = pd.to_numeric(data[\"NEXT_SAC_END_X\"].str.replace(\".\", \"\").str.replace(\",\", \".\")).fillna(0)\n",
    "    data[\"NEXT_SAC_END_Y\"] = pd.to_numeric(data[\"NEXT_SAC_END_Y\"].str.replace(\".\", \"\").str.replace(\",\", \".\")).fillna(0)\n",
    "    data[\"NEXT_SAC_DURATION\"] = pd.to_numeric(data[\"NEXT_SAC_DURATION\"].str.replace(\".\", \"\").str.replace(\",\", \".\")).fillna(0)\n",
    "    data[\"NEXT_SAC_BLINK_START\"] = pd.to_numeric(data[\"NEXT_SAC_BLINK_START\"].str.replace(\".\", \"\")).fillna(0)\n",
    "    data[\"NEXT_SAC_BLINK_END\"] = pd.to_numeric(data[\"NEXT_SAC_BLINK_END\"].str.replace(\".\", \"\")).fillna(0)\n",
    "    data[\"NEXT_SAC_ANGLE\"] = pd.to_numeric(data[\"NEXT_SAC_ANGLE\"].str.replace(\".\", \"\").str.replace(\",\", \".\")).fillna(0)\n",
    "    data[\"NEXT_SAC_AVG_VELOCITY\"] = pd.to_numeric(data[\"NEXT_SAC_AVG_VELOCITY\"].str.replace(\".\", \"\").str.replace(\",\", \".\")).fillna(0)\n",
    "    data[\"NEXT_SAC_BLINK_DURATION\"] = pd.to_numeric(data[\"NEXT_SAC_BLINK_DURATION\"].str.replace(\".\", \"\").str.replace(\",\", \".\")).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scroll data from json file\n",
    "def extract_scroll(mrs_json, idx):\n",
    "    # There is two key format: scroll|mouse-website_id-part_id or scroll|mouse-website_id\n",
    "    # So we need to check that out\n",
    "    r = re.compile(\"scroll-\"+str(idx)+\"(?!\\d)\")\n",
    "\n",
    "    for item in mrs_json:\n",
    "        match = list(filter(r.match, list(item.keys())))\n",
    "        \n",
    "        if(len(match) > 0):\n",
    "            return pd.DataFrame(item[match[0]])\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mouse data from json file\n",
    "def extract_mouse(mrs_json, idx):\n",
    "    # There is two key format: scroll|mouse-website_id-part_id or scroll|mouse-website_id\n",
    "    # So we need to check that out\n",
    "    r = re.compile(\"mouse-\"+str(idx)+\"(?!\\d)\")\n",
    "\n",
    "    for item in mrs_json:\n",
    "        match = list(filter(r.match, list(item.keys())))\n",
    "        \n",
    "        if(len(match) > 0):\n",
    "            return pd.DataFrame(item[match[0]])\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract message datetime in a new column\n",
    "def split_equal(row):\n",
    "    string = row[\"CURRENT_MSG_TEXT\"].split(\"=\")\n",
    "    row[\"EVENT_NAME\"] = string[0].strip()\n",
    "    row[\"EVENT_DATETIME\"] = string[1].strip()\n",
    "    \n",
    "    del row[\"CURRENT_MSG_TEXT\"]\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time sync\n",
    "def sync_time(cell, msg):\n",
    "    timedelta = cell - msg[\"CURRENT_MSG_TIME\"][0]\n",
    "    to_return = msg.loc[0, \"EVENT_DATETIME\"] + pd.Timedelta(milliseconds=timedelta)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_and_clean(group):\n",
    "    trial_index = group['TRIAL_INDEX'].unique()[0]\n",
    "    \n",
    "    msg_start_trial = msg.query(\"TRIAL_INDEX == @trial_index and EVENT_NAME == 'TRIAL START'\").reset_index(drop=True)\n",
    "    msg_end_trial = msg.query(\"TRIAL_INDEX == @trial_index and EVENT_NAME == 'TRIAL END'\").reset_index(drop=True)\n",
    "\n",
    "    group[\"DATETIME\"] = group[\"CURRENT_FIX_START\"].apply(lambda x: sync_time(x, msg_start_trial))\n",
    "    \n",
    "    group = group[group[\"DATETIME\"] > msg_start_trial[\"EVENT_DATETIME\"][0]]\n",
    "    group = group[group[\"DATETIME\"] < msg_end_trial[\"EVENT_DATETIME\"][0]]\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website_id(trial_num):\n",
    "    return config[\"rand_weblist\"][trial_num - 1][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_offset(trial_scroll, date_eye):\n",
    "    result = trial_scroll[trial_scroll[\"datetime\"] < date_eye]\n",
    "    if(result.empty):\n",
    "        return 0\n",
    "    else:\n",
    "        return result.iloc[-1][\"offset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset(group):\n",
    "    website_id = get_website_id(group['TRIAL_INDEX'].unique()[0])\n",
    "    group[\"OFFSET\"] = group[\"DATETIME\"].apply(lambda x: get_last_offset(df_scroll.query(\"website_id == \"+str(website_id)), x))\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition 1:  Free    + NoPub\n",
    "# Condition 2:  Target  + NoPub\n",
    "# Condition 3:  Free    + Skin\n",
    "# Condition 4:  Target  + Skin\n",
    "# Condition 5:  Free    + Skin/MPU\n",
    "# Condition 6:  Target  + Skin/MPU\n",
    "def get_condition(trial_num):\n",
    "    data = config[\"rand_weblist\"][trial_num - 1]\n",
    "    if(data[\"type\"] == \"free\" and data[\"ad_id\"] == 0 and data[\"mpu_id\"] == 0):\n",
    "        return 1\n",
    "    elif(data[\"type\"] == \"target\" and data[\"ad_id\"] == 0 and data[\"mpu_id\"] == 0):\n",
    "        return 2\n",
    "    elif(data[\"type\"] == \"free\" and data[\"ad_id\"] > 0 and data[\"mpu_id\"] == 0):\n",
    "        return 3\n",
    "    elif(data[\"type\"] == \"target\" and data[\"ad_id\"] > 0 and data[\"mpu_id\"] == 0):\n",
    "        return 4\n",
    "    elif(data[\"type\"] == \"free\" and data[\"ad_id\"] > 0 and data[\"mpu_id\"] > 0):\n",
    "        return 5\n",
    "    elif(data[\"type\"] == \"target\" and data[\"ad_id\"] > 0 and data[\"mpu_id\"] > 0):\n",
    "        return 6\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_order(group):\n",
    "    size = group.shape[0]\n",
    "    group[\"ORDER\"] = np.arange(1, size+1)\n",
    "    return group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../data/\"\n",
    "file_suffix = \"_raw.csv\"\n",
    "export_suffix = \"_built.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data\\part_1\\part_1_raw.csv\n",
      "--- Exporting to ../data\\part_1\\part_1_built.csv\n",
      "--- Exporting to ../data\\part_1\\part_1_scroll.csv\n",
      "--- Exporting to ../data\\part_1\\part_1_mouse_browser.csv\n",
      "Processing ../data\\part_2\\part_2_raw.csv\n",
      "--- Exporting to ../data\\part_2\\part_2_built.csv\n",
      "--- Exporting to ../data\\part_2\\part_2_scroll.csv\n",
      "--- Exporting to ../data\\part_2\\part_2_mouse_browser.csv\n",
      "Processing ../data\\part_3\\part_3_raw.csv\n",
      "--- Exporting to ../data\\part_3\\part_3_built.csv\n",
      "--- Exporting to ../data\\part_3\\part_3_scroll.csv\n",
      "--- Exporting to ../data\\part_3\\part_3_mouse_browser.csv\n",
      "Processing ../data\\part_4\\part_4_raw.csv\n",
      "--- Exporting to ../data\\part_4\\part_4_built.csv\n",
      "--- Exporting to ../data\\part_4\\part_4_scroll.csv\n",
      "--- Exporting to ../data\\part_4\\part_4_mouse_browser.csv\n",
      "Processing ../data\\part_7\\part_7_raw.csv\n",
      "--- Exporting to ../data\\part_7\\part_7_built.csv\n",
      "--- Exporting to ../data\\part_7\\part_7_scroll.csv\n",
      "--- Exporting to ../data\\part_7\\part_7_mouse_browser.csv\n",
      "Processing ../data\\part_709\\part_709_raw.csv\n",
      "--- Exporting to ../data\\part_709\\part_709_built.csv\n",
      "--- Exporting to ../data\\part_709\\part_709_scroll.csv\n",
      "--- Exporting to ../data\\part_709\\part_709_mouse_browser.csv\n",
      "Processing ../data\\part_710\\part_710_raw.csv\n",
      "--- Exporting to ../data\\part_710\\part_710_built.csv\n",
      "--- Exporting to ../data\\part_710\\part_710_scroll.csv\n",
      "--- Exporting to ../data\\part_710\\part_710_mouse_browser.csv\n",
      "Processing ../data\\part_711\\part_711_raw.csv\n",
      "--- Exporting to ../data\\part_711\\part_711_built.csv\n"
     ]
    }
   ],
   "source": [
    "for path in glob.glob(root_dir+\"**/*\"+file_suffix):\n",
    "    print(\"Processing {}\".format(path))\n",
    "    path_dir = os.path.dirname(path)\n",
    "    \n",
    "    ################ LOAD\n",
    "    \n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    clean_columns(df)\n",
    "    msg = pd.read_csv(path.replace(file_suffix, \"_msg.csv\"), sep=\"\\t\")\n",
    "    \n",
    "    part_id = int(path_dir.split(\"_\")[1])\n",
    "    \n",
    "    mrs_json = json.load(open(path_dir+\"/records-\"+str(part_id)+\".mrs\"))\n",
    "    config = pickle.load(open(path.replace(file_suffix, \".cfg\"), 'rb'))\n",
    "    \n",
    "    df[\"PART_ID\"] = part_id\n",
    "    \n",
    "    ################ SCROLL and MOUSE\n",
    "    \n",
    "    # Build scroll offset and mouse dataset\n",
    "    df_scroll = pd.DataFrame()\n",
    "    df_mouse = pd.DataFrame()\n",
    "    for i in range(1,19):\n",
    "        scroll = None\n",
    "        mouse = None\n",
    "        scroll = extract_scroll(mrs_json, i)\n",
    "        mouse = extract_mouse(mrs_json, i)\n",
    "        scroll[\"website_id\"] = i\n",
    "        mouse[\"website_id\"] = i\n",
    "        df_scroll = pd.concat([df_scroll, scroll])\n",
    "        df_mouse = pd.concat([df_mouse, mouse])\n",
    "\n",
    "    # Extract right Datetime\n",
    "    # Timestamp is gave by `new Date().getTime()` in Javascript which is in ms\n",
    "    # And since this same function give UTC time, we need to add 1H\n",
    "    \n",
    "    time_to_add = 0\n",
    "    # There was a time change on 25 March 2018, so the time shift between the datasets is not 1h anymore but 2h\n",
    "    if(part_id > 700 and part_id < 712):\n",
    "        time_to_add = 1\n",
    "    else:\n",
    "        time_to_add = 2\n",
    "    \n",
    "    df_scroll[\"datetime\"] = pd.to_datetime(df_scroll[\"timestamp\"], unit=\"ms\") + pd.Timedelta(hours=time_to_add)\n",
    "    df_mouse[\"datetime\"] = pd.to_datetime(df_mouse[\"timestamp\"], unit=\"ms\") + pd.Timedelta(hours=time_to_add)\n",
    "    \n",
    "    \n",
    "    ################ MESSAGE\n",
    "    \n",
    "    # Delete useless msg\n",
    "    msg = msg[~msg[\"CURRENT_MSG_TEXT\"].str.contains(\"!MODE RECORD\")].reset_index(drop=True)\n",
    "    # Split columns\n",
    "    msg = msg.apply(split_equal, axis=1)\n",
    "    # Convert to Datetime\n",
    "    try:\n",
    "        msg[\"EVENT_DATETIME\"] = pd.to_datetime(msg[\"EVENT_DATETIME\"])\n",
    "    except ValueError:\n",
    "        msg[\"EVENT_DATETIME\"] = pd.to_datetime(msg[\"EVENT_DATETIME\"], format=\"%Y-%m-%d %H:%M:%S %f\")\n",
    "\n",
    "    \n",
    "    ################ EXPORT\n",
    "    \n",
    "    if(not os.path.isfile(path.replace(file_suffix, export_suffix)) or overwrite):\n",
    "        print(\"--- Exporting to {}\".format(path.replace(file_suffix, export_suffix)))\n",
    "        \n",
    "        ################ TIME SYNC\n",
    "    \n",
    "        df = df.groupby(\"TRIAL_INDEX\").apply(sync_and_clean).reset_index(drop = True)\n",
    "\n",
    "\n",
    "        ################ OFFSET\n",
    "\n",
    "        df = df.groupby(\"TRIAL_INDEX\").apply(get_offset)\n",
    "        df[\"Y_OFFSET\"] = df[\"CURRENT_FIX_Y\"] + df[\"OFFSET\"]\n",
    "\n",
    "\n",
    "        ################ WEBSITES AND CONDITIONS\n",
    "\n",
    "        df[\"WEBSITE_ID\"] = df[\"TRIAL_INDEX\"].apply(get_website_id)\n",
    "        df[\"CONDITION\"] = df[\"TRIAL_INDEX\"].apply(get_condition)\n",
    "        \n",
    "        \n",
    "        ################ TASKS\n",
    "        \n",
    "        df[\"TASK\"] = 'Target'\n",
    "        df.loc[df.query(\"CONDITION in [1,3,5]\").index, \"TASK\"] = \"Free\"\n",
    "\n",
    "        \n",
    "        ################ DISTRACTORS\n",
    "        \n",
    "        df[\"DISTRACTOR\"] = 'NoAd'\n",
    "        df.loc[df.query(\"CONDITION in [3,4]\").index, \"DISTRACTOR\"] = \"Skin\"\n",
    "        df.loc[df.query(\"CONDITION in [5,6]\").index, \"DISTRACTOR\"] = \"SkinMpu\"\n",
    "        \n",
    "        \n",
    "        ################ ORDER\n",
    "        \n",
    "        df = df.groupby(\"TRIAL_INDEX\").apply(generate_order)\n",
    "        \n",
    "        df.to_csv(path.replace(file_suffix, export_suffix), index=False)\n",
    "    \n",
    "    else:\n",
    "        print(\"--- Skipping export\")\n",
    "        \n",
    "    if(not os.path.isfile(path.replace(file_suffix, \"_scroll.csv\")) or overwrite):\n",
    "        df_scroll.to_csv(path.replace(file_suffix, \"_scroll.csv\"),index=False)\n",
    "        print(\"--- Exporting to {}\".format(path.replace(file_suffix, \"_scroll.csv\")))\n",
    "    else:\n",
    "        print(\"--- Skipping export\")\n",
    "        \n",
    "    if(not os.path.isfile(path.replace(file_suffix, \"_mouse_browser.csv\")) or overwrite):\n",
    "        df_mouse.to_csv(path.replace(file_suffix, \"_mouse_browser.csv\"),index=False)\n",
    "        print(\"--- Exporting to {}\".format(path.replace(file_suffix, \"_mouse_browser.csv\")))\n",
    "    else:\n",
    "        print(\"--- Skipping export\")\n",
    "    \n",
    "    #df = None\n",
    "    msg = None\n",
    "    df_scroll = None\n",
    "    df_mouse = None\n",
    "    mrs_json = None\n",
    "    config = None\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
